{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Recurrent Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is a natural language processing task that involves determining the sentiment expressed in a piece of text. This project focuses on building a sentiment analysis model using Recurrent Neural Networks (RNNs) for classifying movie reviews as either positive or negative.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The primary goal of this project is to create a model capable of accurately predicting sentiment in movie reviews. The model is trained on a dataset of labeled movie reviews, learning the nuances of language and context associated with positive and negative sentiments.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The sentiment analysis model is constructed using an RNN architecture, specifically a Long Short-Term Memory (LSTM) network. LSTMs are well-suited for sequence data, making them effective for capturing dependencies and patterns in textual information over time. The architecture includes an embedding layer for word representation, LSTM layers for sequence modeling, and a fully connected layer with a sigmoid activation for binary classification.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset used for training and evaluation consists of movie reviews labeled with sentiment polarity. Each review is preprocessed and tokenized to create a numerical representation that the model can process. The dataset is split into training, validation, and test sets to ensure robust evaluation of the model's performance.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The model is trained using a binary cross-entropy loss function and optimized with the Adam optimizer. During training, the model undergoes backpropagation to adjust its parameters based on the calculated loss. To prevent overfitting, dropout layers are incorporated. The training process is monitored through both training and validation loss.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The trained model is evaluated on a separate test set to assess its generalization to unseen data. Performance metrics such as test loss and accuracy are calculated to gauge the effectiveness of the sentiment analysis model.\n",
    "\n",
    "### Inference on New Reviews\n",
    "\n",
    "The notebook includes a function for making predictions on new reviews. Users can input a movie review, and the model will provide a sentiment prediction, indicating whether the review is likely positive or negative.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "\n",
    "# Read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "# Print first 2000 characters in reviews\n",
    "print(reviews[:2000])\n",
    "print()\n",
    "\n",
    "# Print first 20 characters in labels\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Next I import the 'punctuation' module from the 'string' library and print a list of punctuation characters. Subsequently, I process the previously loaded 'reviews' data by converting it to lowercase and removing all punctuation (punctuation has no bearing on whether review is pos or neg). The cleaned text is stored in the variable 'all_text.' In essence, this code prepares the textual data for further analysis by standardizing the case and eliminating punctuation from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from string import punctuation\n",
    "\n",
    "# Print list of punctuation characters\n",
    "print(punctuation)\n",
    "\n",
    "# Clean up the reviews: lowercase, remove punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation]) # get rid of punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, the cleaned text stored in 'all_text' is further processed. It first splits the text into a list of strings based on new lines and spaces, creating a list called 'reviews_split.' Then, it joins these strings back together into one long string, stored again in 'all_text.' The final step involves creating a list of words by splitting the long string using spaces, and this list is stored in the variable 'words.' Essentially, this code manipulates the text data to organize it into a format suitable for subsequent analysis, breaking it down into lines, rejoining it, and then extracting individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n') # split by new lines and spaces\n",
    "all_text = ' '.join(reviews_split) # join all the reviews together into one long string\n",
    "\n",
    "# Create a list of words\n",
    "words = all_text.split() # split long string by spaces into words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i print out the first 30 words to inspect whether the pre-processing was sussessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate pre-processing on first 30 words\n",
    "words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words (Tokenizing)\n",
    "\n",
    "This code segment focuses on building a vocabulary and tokenizing the reviews. It uses the 'Counter' class from the 'collections' library to create a dictionary ('counts') mapping words to their frequencies in the 'words' list. The vocabulary is then established by sorting the words based on frequency. Each word is assigned a unique integer, excluding 0 for padding purposes, forming a dictionary named 'vocab_to_int.' The code subsequently tokenizes each review in the 'reviews_split' list by mapping the words to their corresponding integers using 'vocab_to_int,' and the results are stored in 'reviews_ints.' Finally, the code prints statistics about the vocabulary size and displays the tokenized representation of the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  74072\n",
      "\n",
      "Tokenized review: \n",
      " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers (do not use 0 as it is used for padding later on)\n",
    "counts = Counter(words) # create a dictionary of word counts; maps most common words to smallest integers\n",
    "vocab = sorted(counts, key=counts.get, reverse=True) # sort words by frequency/commonality\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} # for each word in the vocab list, it will return a tuple containing the index and the word\n",
    "\n",
    "# Alternative way to create vocab_to_int\n",
    "# words_unique = np.unique(words)\n",
    "# vocab_to_int = dict(zip(words_unique, range(1, len(words_unique)+1)))\n",
    "\n",
    "# Use the dict to tokenize each review in reviews_split\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    word_token = [vocab_to_int[word] for word in review.split()] # tokenize each word\n",
    "    reviews_ints.append(word_token) # append tokenized word to reviews_ints\n",
    "\n",
    "# Print stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# Print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels (Tokenizing)\n",
    "\n",
    "This code section deals with encoding labels for sentiment analysis. It splits the 'labels' string at new lines, creating a list called 'labels_split.' Then, it encodes the labels into numerical values, assigning 1 for 'positive' and 0 for 'negative.' The encoded labels are stored in the 'encoded_labels' numpy array. To verify the correctness of the encoding, the code prints the original and encoded labels for the first 10 samples. This process prepares the sentiment labels in a format suitable for machine learning models, where positive sentiments are represented by 1 and negative sentiments by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "labels_split = labels.split('\\n') # split at new lines\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split]) # convert 'positive' to 1 and 'negative' to 0\n",
    "\n",
    "# Check if labels are correct\n",
    "for i in range(10):\n",
    "    print(f' Label: {labels_split[i]}, Encoded label: {encoded_labels[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers\n",
    "\n",
    "This code snippet focuses on analyzing the lengths of tokenized reviews. It uses the 'Counter' class to create a dictionary ('review_lens') that maps the lengths of the tokenized reviews to their frequencies. The code then prints the number of zero-length reviews and the maximum review length in the dataset. This analysis provides insights into the distribution of review lengths, identifying any anomalies such as zero-length reviews and the longest review in terms of the number of tokens. Understanding the distribution of review lengths is crucial for identifying outliers in preprocessing and setting appropriate parameters when training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "# Outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_ints]) # Dict of review lengths and their frequency\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet addresses the removal of outlier reviews with zero length. It starts by printing the initial number of reviews in the 'reviews_ints' list. Then, it identifies the indices of reviews with a length of zero using list comprehension and removes those entries from both the 'reviews_ints' and 'encoded_labels' lists. Finally, the code prints the updated number of reviews after the removal of outliers. This process ensures that any reviews with no content are excluded from the dataset, maintaining data integrity and preparing the data for further analysis or machine learning model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "# Print num reviews before removing outliers\n",
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "# Remove any reviews/labels with zero length from the reviews_ints list.\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0] # get indices of any reviews with length 0\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx] # remove 0-length reviews \n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx]) # remove 0-length labels\n",
    "\n",
    "# Print num reviews after removing outliers\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding & Truncating sequences\n",
    "\n",
    "The provided code defines a function named `pad_features` that pads or truncates a given list of tokenized reviews (`reviews_ints`) to a specified sequence length (`seq_length`). The function initializes a NumPy array called 'features' with zeros, where the number of rows corresponds to the number of reviews, and the number of columns is set to the desired sequence length. \n",
    "\n",
    "For each review in the input list, the function populates the corresponding row in the 'features' array. If the review is longer than the specified sequence length, it truncates the review to fit. If the review is shorter, it pads the beginning of the row with zeros.\n",
    "\n",
    "In summary, this function ensures that all reviews have a consistent length by either padding them with zeros or truncating them to the desired sequence length, making the data suitable for input to machine learning models with fixed-size inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    # Getting the correct rows x cols shape\n",
    "    # this creats an array of zeros with the shape of the number of reviews by the sequence length\n",
    "    features = np.zeros((len(reviews_ints), # rows = number of reviews\n",
    "                        seq_length), # cols = desired sequence length\n",
    "                        dtype=int)\n",
    "\n",
    "    # For each review, I grab that review and insert it into the array of zeros\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        # if the review is longer than the sequence length, it will be truncated ([:seq_length])\n",
    "        # if the review is shorter than the sequence length, the previously filled in zeros will remain = it will be padded with zeros\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length] # start at the end of the array [i] and work backwards towards the beginning of the array [-len(row):]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, the implemented `pad_features` function is tested. It sets the desired sequence length (`seq_length`) to 200 and applies the function to the tokenized reviews stored in `reviews_ints`. The result is stored in the 'features' variable.\n",
    "\n",
    "The subsequent test statements check whether the number of rows in the 'features' array matches the number of reviews in the original 'reviews_ints' list and whether each row in 'features' has the specified sequence length of 200.\n",
    "\n",
    "Finally, the code prints the first 10 values of the first 30 batches (rows) of the 'features' array to provide a glimpse into the transformed data, demonstrating how the reviews are padded or truncated to the desired sequence length. This step is crucial for preparing the data for training machine learning models with fixed-size input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   330   578    34     3   162   748  2731     9   325]\n",
      " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
      " [    1    20     6    76    40     6    58    81    95     5]\n",
      " [   54    10    84   329 26230 46427    63    10    14   614]\n",
      " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   40    26   109 17952  1422     9     1   327     4   125]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   10   499     1   307 10399    55    74     8    13    30]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Test implementation\n",
    "seq_length = 200\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "# Test statements\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# Print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test\n",
    "\n",
    "This code segment defines a fraction (`split_frac`) to determine the proportion of data to allocate for training. It then calculates the split index based on this fraction, dividing the data into training, validation, and test sets for both features and labels (`train_x`, `val_x`, `test_x`, `train_y`, `val_y`, `test_y`). The split is performed in such a way that 80% of the data is used for training, and the remaining 20% is equally divided between the validation and test sets.\n",
    "\n",
    "Finally, the code prints out the shapes of the resultant feature data for each set, providing information on the number of samples and the sequence length. This step ensures transparency in understanding the distribution of data across training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "# Define fraction to keep for training\n",
    "split_frac = 0.8\n",
    "\n",
    "# Split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = int(len(features)*split_frac) # get index at which to split: number of reviews times the split fraction (80%)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:] # use split index to split features into training and remaining\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5) # get half the indeces to split the remaining data into validation and test\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:] # split all left until the test index into validation and all right into test set\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## Print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching\n",
    "\n",
    "This code snippet demonstrates the preparation of PyTorch DataLoader objects for training, validation, and test datasets. It utilizes the `TensorDataset` and `DataLoader` classes from the `torch.utils.data` module. \n",
    "\n",
    "- Tensor datasets (`train_data`, `valid_data`, `test_data`) are created, converting NumPy arrays to PyTorch tensors.\n",
    "- The batch size for the data loaders is set to 50.\n",
    "- The data is shuffled during loading to prevent the model from learning patterns based on the ordering of the data.\n",
    "\n",
    "These DataLoader objects (`train_loader`, `valid_loader`, `test_loader`) enable efficient loading of batches during model training, validation, and testing. The use of DataLoader facilitates easier iteration over the datasets in mini-batches, which is a common practice in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# Set batch size for data loaders\n",
    "batch_size = 50\n",
    "\n",
    "# Shuffle data, so that model doesn't learn anything about ordering of the data, and instead focuses on content\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, a single batch of training data is obtained using the `iter` function and `next` method on the `train_loader`. The resulting batch consists of both features (`sample_x`) and labels (`sample_y`). The code then prints the shapes and contents of the sampled input features and labels.\n",
    "\n",
    "This step provides a quick check on the dimensions of a batch, ensuring that the data loading process is functioning as expected. Understanding the shapes of input features and labels in a batch is crucial for designing and debugging deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[   1, 2797, 2912,  ..., 9251,    4, 1374],\n",
      "        [   0,    0,    0,  ...,   35, 1069,  226],\n",
      "        [   0,    0,    0,  ...,   43,   44,    8],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1511,   71,  350],\n",
      "        [   0,    0,    0,  ...,   16,  339,  572],\n",
      "        [   0,    0,    0,  ...,   39,   10,  120]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "# Print shape of batch of features and labels\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sentiment Network with PyTorch\n",
    "\n",
    "This code snippet checks whether a GPU is available for training by using the `torch.cuda.is_available()` function. If a GPU is available, it prints \"Training on GPU.\" Otherwise, it prints \"No GPU available, training on CPU.\"\n",
    "\n",
    "This check is important as training deep learning models on GPUs can significantly accelerate the training process compared to using CPUs. It allows for parallel processing and handling large-scale computations more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a SentimentRNN class, which is a recurrent neural network (RNN) model for sentiment analysis. The model is built using PyTorch's `nn.Module` class. Here's a breakdown of the key components:\n",
    "\n",
    "- **Embedding Layer:** Converts word indices to dense vectors of fixed size (`embedding_dim`). Need to add this layer because there are 74000+ words in our vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. I could have also trained an embedding layer using Word2Vec, then load it here. Instead, I simply add an embedding layer here, using it for only dimensionality reduction, and let the network learn the weights.\n",
    "- **LSTM Layer:** Processes the embedded input sequences, capturing sequential dependencies.\n",
    "- **Dropout Layer:** Introduces regularization by randomly setting a fraction of input units to zero during training.\n",
    "- **Fully-Connected Layer:** Reduces the dimensionality to the output size (`output_size`).\n",
    "- **Sigmoid Activation Function:** Converts the output to a probability between 0 and 1.\n",
    "\n",
    "The `forward` method defines how input is processed through the layers during a forward pass. The `init_hidden` method initializes the hidden state of the LSTM layers.\n",
    "\n",
    "The model is designed to work with GPU acceleration if available (`train_on_gpu` is `True`). The embedding layer is used to convert word indices to vectors, and the LSTM processes these vectors sequentially. The final output is obtained by passing through dropout, a fully-connected layer, and a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch.nn as nn\n",
    "\n",
    "# Defining the model\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        # Defining parameters for the model\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding words for dimensionality reduction (altnernative to one-hot encoding, which is inefficient for large vocabularies)\n",
    "        # creates a lookup table that maps words to vectors of a specified size\n",
    "        self.embedding = nn.Embedding(vocab_size, # number of rows: word integers\n",
    "                                      embedding_dim) # number of columns: the embedding dimension\n",
    "        \n",
    "        # Defining the LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, # input size is equal to the output of the embedding layer (i.e. embedding dimension)\n",
    "                            hidden_dim, # output size is equal to the hidden dimension/number of units in the hidden layer\n",
    "                            n_layers,\n",
    "                            dropout=drop_prob, \n",
    "                            batch_first=True) # True because we are using DataLoaders to batch our data like that\n",
    "        \n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Define linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        # Extracting the size of the first dimension of the tensor x and assigning it to the variable batch_size\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Embed text and pass through LSTM layer\n",
    "        x = x.long() # converts the elements of the tensor x to a 64-bit integer data type (torch.int64 in the case of PyTorch).\n",
    "        embeds = self.embedding(x) # run the input x through the embedding layer\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden) # run the embedding output through the LSTM layer\n",
    "        \n",
    "        # Stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Pass through dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out) # run the LSTM output through the dropout layer\n",
    "        out = self.fc(out) # run the dropout output through the fully-connected layer\n",
    "        \n",
    "        # Activate: Sigmoid function\n",
    "        sig_out = self.sig(out) # run the fully-connected output through the sigmoid function\n",
    "        \n",
    "        # Reshape to be batch_size first (to feed into next LSTM layer)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # Return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "This code segment instantiates an object of the `SentimentRNN` class with specified hyperparameters. The model is configured with the following hyperparameters:\n",
    "\n",
    "- `vocab_size`: The size of the vocabulary plus 1 for zero-padding.\n",
    "- `output_size`: 1, representing the one output neuron for positive/negative sentiment classification.\n",
    "- `embedding_dim`: 400, indicating the dimensionality of the word embeddings.\n",
    "- `hidden_dim`: 256, specifying the number of hidden features in the LSTM layers.\n",
    "- `n_layers`: 2, indicating the number of LSTM layers.\n",
    "\n",
    "The instantiated model is named `net`, and the `print(net)` statement is used to display the architecture of the model, providing information about its layers and parameters. This model is suitable for sentiment analysis tasks on the provided dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1 # one output neuron/class score for positive/negative\n",
    "embedding_dim = 400 # just a smaller representation of vocabulary of 70k words --> any value between like 200 and 500 should work\n",
    "hidden_dim = 256 # 256 hidden features should be enough to distinguish between positive and negative reviews.\n",
    "n_layers = 2 # 2 layers should be enough to distinguish between positive and negative reviews.\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this code snippet, the learning rate (`lr`) is defined and the loss function and optimizer are set up for training the sentiment analysis model.\n",
    "\n",
    "- `criterion`: Binary Cross Entropy Loss (`nn.BCELoss()`), a suitable choice for binary classification problems like sentiment analysis where the goal is to predict whether the sentiment is positive or negative. This loss function applies cross-entropy to a single value between 0 and 1.\n",
    "\n",
    "- `optimizer`: Adam optimizer (`torch.optim.Adam`) is chosen, which is a popular optimization algorithm. It is used to update the parameters of the neural network during training. The optimizer is applied to the model's parameters (`net.parameters()`) with the specified learning rate (`lr`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "lr=0.001\n",
    "\n",
    "# Set loss and optimization functions\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss, applies cross entropy loss to a single value between 0 and 1.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet trains the sentiment analysis model for a specified number of epochs (`epochs`). It iterates through the training data in batches, calculating and backpropagating the loss to update the model parameters. Some key components of the training loop include:\n",
    "\n",
    "- **Hidden State Initialization:** The hidden state (`h`) is initialized at the beginning of each epoch and for each batch.\n",
    "- **GPU Acceleration:** If a GPU is available (`train_on_gpu` is `True`), the inputs and labels are moved to the GPU.\n",
    "- **Gradient Clipping:** The `clip_grad_norm_` function is used to prevent the exploding gradient problem in RNNs/LSTMs by scaling gradients during backpropagation.\n",
    "- **Validation Loss Calculation:** Every `print_every` batches, the model is evaluated on the validation set (`valid_loader`) to calculate the validation loss (`val_loss`). The average validation loss is printed along with the current training loss.\n",
    "- **Model Mode Switching:** The model is switched between training and evaluation modes using `net.train()` and `net.eval()` to ensure correct behavior of layers like dropout during training and evaluation.\n",
    "\n",
    "This training loop is crucial for training the sentiment analysis model and monitoring its performance on both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Train Loss: 0.647868... Val Loss: 0.641024\n",
      "Epoch: 1/4... Step: 200... Train Loss: 0.576166... Val Loss: 0.598000\n",
      "Epoch: 1/4... Step: 300... Train Loss: 0.555865... Val Loss: 0.621067\n",
      "Epoch: 1/4... Step: 400... Train Loss: 0.604562... Val Loss: 0.549225\n",
      "Epoch: 2/4... Step: 500... Train Loss: 0.284617... Val Loss: 0.516605\n",
      "Epoch: 2/4... Step: 600... Train Loss: 0.506833... Val Loss: 0.491125\n",
      "Epoch: 2/4... Step: 700... Train Loss: 0.505795... Val Loss: 0.485445\n",
      "Epoch: 2/4... Step: 800... Train Loss: 0.321016... Val Loss: 0.449118\n",
      "Epoch: 3/4... Step: 900... Train Loss: 0.244150... Val Loss: 0.593418\n",
      "Epoch: 3/4... Step: 1000... Train Loss: 0.350111... Val Loss: 0.509438\n",
      "Epoch: 3/4... Step: 1100... Train Loss: 0.247316... Val Loss: 0.442915\n",
      "Epoch: 3/4... Step: 1200... Train Loss: 0.268553... Val Loss: 0.433219\n",
      "Epoch: 4/4... Step: 1300... Train Loss: 0.303633... Val Loss: 0.515278\n",
      "Epoch: 4/4... Step: 1400... Train Loss: 0.311155... Val Loss: 0.526051\n",
      "Epoch: 4/4... Step: 1500... Train Loss: 0.194953... Val Loss: 0.505703\n",
      "Epoch: 4/4... Step: 1600... Train Loss: 0.287554... Val Loss: 0.567540\n"
     ]
    }
   ],
   "source": [
    "# Set training params\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# Move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "# Set model to train mode\n",
    "net.train()\n",
    "\n",
    "# Train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # Initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # Batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise this would backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # Clear the gradients of all optimized variables\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # Calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float()) # making sure that our outputs are squeezed so that they do not have an empty dimension\n",
    "        loss.backward() # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip) # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        optimizer.step() # Perform a single optimization step (parameter update)\n",
    "\n",
    "        # Get loss stats every few batches\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "\n",
    "            # Set model to evaluation mode\n",
    "            net.eval()\n",
    "\n",
    "            for inputs, labels in valid_loader:\n",
    "                # Creating new variables for the hidden state, otherwise this would backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                # Get the output from the model\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())  # making sure that our outputs are squeezed so that they do not have an empty dimension\n",
    "                val_losses.append(val_loss.item()) # append the validation loss to the list of validation losses\n",
    "\n",
    "            # Set model back to train mode\n",
    "            net.train()\n",
    "\n",
    "            # Print training and validation loss stats\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Train Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "This code segment evaluates the trained sentiment analysis model on the test set and prints out relevant statistics, including the average test loss and test accuracy. Here's a breakdown of the key components:\n",
    "\n",
    "- **Loss Calculation:** For each batch in the test set, the model's output is compared to the true labels to calculate the test loss (`test_loss`).\n",
    "- **Accuracy Calculation:** The number of correct predictions is tracked, and the overall test accuracy is computed as the ratio of correct predictions to the total number of test samples.\n",
    "- **Conversion to Numpy:** If running on GPU (`train_on_gpu` is `True`), the predictions and correctness tensor are moved to the CPU before converting to NumPy arrays.\n",
    "- **Printing Statistics:** The average test loss and test accuracy are printed to provide insights into the model's performance on unseen data.\n",
    "\n",
    "This evaluation step is essential for assessing how well the trained model generalizes to new, unseen examples and gives an indication of its overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.558\n",
      "Test accuracy: 0.796\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# Init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# Iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise this would backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # Get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # Calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # Convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # Compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "## Print stats\n",
    "# Avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# Accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a test review\n",
    "\n",
    "First I define a test review for early testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I define the `tokenize_review` function, which processes a given test review by performing the following steps:\n",
    "\n",
    "1. Convert the review to lowercase.\n",
    "2. Remove punctuation from the review.\n",
    "3. Split the processed text into individual words.\n",
    "4. Tokenize the words using the `vocab_to_int` dictionary, where each word is mapped to its corresponding integer index. If a word is not present in the vocabulary, it is assigned an index of 0.\n",
    "\n",
    "The resulting `test_ints` list contains the tokenized representation of the negative test review provided. It represents the integer indices of the words in the review based on the vocabulary used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 247, 18, 10, 28, 108, 113, 14, 388, 2, 10, 181, 60, 273, 144, 11, 18, 68, 76, 113, 2, 1, 410, 14, 539]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# Define function to tokenize review\n",
    "def tokenize_review(test_review):\n",
    "    # Coverting test review to lowercase\n",
    "    test_review = test_review.lower()\n",
    "\n",
    "    # Getting rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # Splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # Tokenizing words using the vocab_to_int dictionary\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int.get(word, 0) for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# Tokenize test review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining a comprehensive function in the final step, here, I will first test out, whether all functions work as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   1 247  18  10  28\n",
      "  108 113  14 388   2  10 181  60 273 144  11  18  68  76 113   2   1 410\n",
      "   14 539]]\n"
     ]
    }
   ],
   "source": [
    "# Test padding of test review\n",
    "seq_length=200 # use the same sequence length that was trained on\n",
    "features = pad_features(test_ints, seq_length)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# Test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` function takes a trained model (`net`), a test review (`test_review`), and an optional parameter for the sequence length (`sequence_length`). It performs the following steps:\n",
    "\n",
    "1. Sets the model to evaluation mode.\n",
    "2. Tokenizes the test review using the `tokenize_review` function.\n",
    "3. Pads the tokenized sequence to the specified sequence length.\n",
    "4. Converts the padded sequence to a PyTorch tensor.\n",
    "5. Initializes the hidden state.\n",
    "6. If a GPU is available, moves the tensor to the GPU.\n",
    "7. Obtains the model's output.\n",
    "8. Rounds the output to the nearest integer (0 or 1).\n",
    "9. Prints the raw prediction value before rounding.\n",
    "10. Outputs a custom response based on whether the rounded prediction is 1 (positive) or 0 (negative).\n",
    "\n",
    "This function provides a convenient way to use the trained model to predict the sentiment of a given test review and print a custom response indicating whether the review is predicted to be positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print custom response based on whether test_review is pos/neg\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    ''' Prints out whether a give review is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "        \n",
    "        params:\n",
    "        net - A trained net \n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # Pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # Convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    # Get batch size from feature tensor\n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # Get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # Convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "\n",
    "    # Printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # Print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing predict function on both negative and positive test reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.004021\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "# Define negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "\n",
    "# Call function on negative test review\n",
    "seq_length=200\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.993114\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "# Define positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "\n",
    "# Call function on negative test review\n",
    "seq_length=200\n",
    "predict(net, test_review_pos, seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
